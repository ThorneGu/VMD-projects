{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PyEMD import EMD\n",
    "import numpy as np\n",
    "\n",
    "labels = []\n",
    "all_files = []\n",
    "path = 'D:/vibration'\n",
    "labels_list = os.listdir(path)\n",
    "print(labels_list)        \n",
    "for i in labels_list:\n",
    "    wavs_path = os.path.join(path,i)\n",
    "    second_dirs = os.listdir(wavs_path)\n",
    "    for csv in second_dirs:\n",
    "        labels.append(i)\n",
    "        csvFilePath = wavs_path+'/'+csv\n",
    "        all_files.append(csvFilePath)\n",
    "print(labels)\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from vmdpy import VMD\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import os\n",
    "\n",
    "path = 'D:/vibration'\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        labels = []\n",
    "        path = 'D:/vibration'\n",
    "        labels_list = os.listdir(path)\n",
    "        for i in labels_list:\n",
    "            wavs_path = os.path.join(path, i)\n",
    "            second_dirs = os.listdir(wavs_path)\n",
    "            for csv in second_dirs:\n",
    "                labels.append(i)\n",
    "        self.label = np.array(labels)\n",
    "        # 创建 OneHotEncoder 对象\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        # 对标签进行独热编码\n",
    "        encoded_labels = self.encoder.fit_transform([[label] for label in self.label])\n",
    "        # 将独热编码结果保存为 Tensor\n",
    "        self.encoded_labels_tensor = torch.tensor(encoded_labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_labels_tensor)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        Fs=1024 # 采样频率\n",
    "        N=1024 # 采样点数\n",
    "        t=np.arange(1,N+1)/N\n",
    "        fre_axis=np.linspace(0,Fs/2,int(N/2))\n",
    "        label = self.encoded_labels_tensor[index]\n",
    "        alpha=2000\n",
    "        tau=0 # tau 噪声容限，即允许重构后的信号与原始信号有差别。\n",
    "        K=3 # K 分解模态（IMF）个数\n",
    "        DC=0 # DC 若为0则让第一个IMF为直流分量/趋势向量\n",
    "        init=1 # init 指每个IMF的中心频率进行初始化。当初始化为1时，进行均匀初始化。\n",
    "        tol=1e-7 # 控制误差大小常量，决定精度与迭代次数\n",
    "        all_files = []\n",
    "        path = 'D:/vibration'\n",
    "        labels_list = os.listdir(path)\n",
    "        for i in labels_list:\n",
    "            wavs_path = os.path.join(path,i)\n",
    "            second_dirs = os.listdir(wavs_path)\n",
    "            for csv in second_dirs:\n",
    "                csvFilePath = wavs_path+'/'+csv\n",
    "                all_files.append(csvFilePath)\n",
    "        csvpath = all_files[index]\n",
    "        data = pd.read_csv(csvpath)\n",
    "        signal = data['0']\n",
    "        signal = np.array(signal)\n",
    "        u, u_hat, omega = VMD(signal, alpha, tau, K, DC, init, tol) # 输出U是各个IMF分量，u_hat是各IMF的频谱，omega为各IMF的中心频率\n",
    "        # 返回加载和预处理后的数据\n",
    "        return torch.as_tensor(u,dtype=torch.float32), torch.as_tensor(label,dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = MyDataset()\n",
    "total_size = len(dataset1)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "print(total_size,train_size,val_size,test_size)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset1, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,label = dataset1[8422]\n",
    "print(data.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,num_classes,num_layers, num_heads, dropout,linear_dim1,linear_dim2):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear = nn.Linear(hidden_dim, input_dim)\n",
    "        self.position_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(hidden_dim, num_layers, num_heads, dropout)\n",
    "        self.MLPclassifier = MLPClassifier(hidden_dim,linear_dim1,linear_dim2,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        encoded = self.position_encoding(x)\n",
    "        transformed = self.transformer_encoder(encoded)\n",
    "        output = self.MLPclassifier(transformed)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_length=3):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        position_encoding = torch.zeros(max_length, hidden_dim)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "\n",
    "        position_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('position_encoding', position_encoding.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.position_encoding[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, num_heads, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.multihead_attention(x, x, x)[0]\n",
    "        x = x + self.dropout(attended)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        x = x + self.dropout(fed_forward)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self,hidden_dim,linear_dim1,linear_dim2,num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(hidden_dim,linear_dim1)\n",
    "        self.linear_2 = nn.Linear(linear_dim1,linear_dim2)\n",
    "        self.linear_3 = nn.Linear(linear_dim2,num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 创建模型实例\n",
    "model = TransformerClassifier(input_dim=1024,\n",
    "                              hidden_dim=1024,\n",
    "                              num_classes=5,\n",
    "                              num_layers=1,\n",
    "                              num_heads=8,\n",
    "                              linear_dim1=500,\n",
    "                              linear_dim2=100,\n",
    "                              dropout=0.1).cuda()\n",
    "for param in model.parameters():\n",
    "    print(type(param), param.size())\n",
    "    \n",
    "num_params = sum([param.nelement() for param in model.parameters()])\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "model = model.to('cuda')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Device: {param.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 5\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # 将模型设置为训练模式\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs, labels = inputs.cuda(),labels.cuda()\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    # 输出每个epoch的训练损失\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_dataloader)}\")\n",
    "\n",
    "    # 验证循环\n",
    "    model.eval() # 将模型设置为评估模式\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_dataloader):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs,1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "        \n",
    "        val_acc = correct_preds / total_preds\n",
    "\n",
    "    # 输出每个epoch的验证损失和准确率\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(val_dataloader)}, Accuracy: {val_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
