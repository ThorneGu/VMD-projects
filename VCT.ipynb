{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "labels = []\n",
    "all_files = []\n",
    "path = 'D:/vibration-3classes'\n",
    "labels_list = os.listdir(path)\n",
    "print(labels_list)        \n",
    "for i in labels_list:\n",
    "    wavs_path = os.path.join(path,i)\n",
    "    second_dirs = os.listdir(wavs_path)\n",
    "    for csv in second_dirs:\n",
    "        labels.append(i)\n",
    "        csvFilePath = wavs_path+'/'+csv\n",
    "        all_files.append(csvFilePath)\n",
    "print(labels)\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmdpy import VMD\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def GetAllFiles(path):\n",
    "    all_files = []\n",
    "    labels_list = os.listdir(path)\n",
    "    for i in labels_list:\n",
    "        wavs_path = os.path.join(path,i)\n",
    "        second_dirs = os.listdir(wavs_path)\n",
    "        for csv in second_dirs:\n",
    "            csvFilePath = wavs_path+'/'+csv\n",
    "            all_files.append(csvFilePath)\n",
    "    return all_files\n",
    "\n",
    "\n",
    "def GetVMD(signal,K):\n",
    "\n",
    "    Fs=1024 # 采样频率\n",
    "    N=1024 # 采样点数\n",
    "    t=np.arange(1,N+1)/N\n",
    "    fre_axis=np.linspace(0,Fs/2,int(N/2))\n",
    "    alpha=1500\n",
    "    tau=0 # tau 噪声容限，即允许重构后的信号与原始信号有差别。\n",
    "    K=3 # K 分解模态（IMF）个数\n",
    "    DC=0 # DC 若为0则让第一个IMF为直流分量/趋势向量\n",
    "    init=1 # init 指每个IMF的中心频率进行初始化。当初始化为1时，进行均匀初始化。\n",
    "    tol=1e-7 # 控制误差大小常量，决定精度与迭代次数\n",
    "    \n",
    "    u, u_hat, omega = VMD(signal, alpha, tau, K, DC, init, tol)\n",
    "    \n",
    "    return u,omega[-1]\n",
    "\n",
    "def GetCWTMembers(signal,freq):\n",
    "    # 采样频率\n",
    "    sampling_rate = 1024\n",
    "    # 尺度长度\n",
    "    totalscal = 1024   \n",
    "    # 小波基函数\n",
    "    wavename = 'morl'\n",
    "    # 小波函数中心频率\n",
    "    fc = pywt.central_frequency(wavename)\n",
    "    # 常数c\n",
    "    cparam = 2 * fc * totalscal  \n",
    "    # 尺度序列\n",
    "    scales = cparam / np.arange(totalscal, 0, -1)\n",
    "    wavelet_center_frequencies = fc / scales\n",
    "    coefficients, frequencies = pywt.cwt(signal, scales, wavename, 1.0/1000)\n",
    "    amp = abs(coefficients)\n",
    "    t = np.linspace(0, 1.0/sampling_rate, sampling_rate, endpoint=False)\n",
    "    # 找到最接近的频率值对应的索引\n",
    "    index = np.argmin(np.abs(wavelet_center_frequencies - freq))\n",
    "    # 提取对应频率的振幅随时间变化数据\n",
    "    amp_at_target_frequency = abs(coefficients[index])\n",
    "\n",
    "    return amp_at_target_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from vmdpy import VMD\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import os\n",
    "\n",
    "path = 'D:/vibration'\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        labels = []\n",
    "        path = 'D:/vibration-3classes'\n",
    "        labels_list = os.listdir(path)\n",
    "        for i in labels_list:\n",
    "            wavs_path = os.path.join(path, i)\n",
    "            second_dirs = os.listdir(wavs_path)\n",
    "            for csv in second_dirs:\n",
    "                labels.append(i)\n",
    "        self.label = np.array(labels)\n",
    "        # 创建 OneHotEncoder 对象\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        # 对标签进行独热编码\n",
    "        encoded_labels = self.encoder.fit_transform([[label] for label in self.label])\n",
    "        # 将独热编码结果保存为 Tensor\n",
    "        self.encoded_labels_tensor = torch.tensor(encoded_labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_labels_tensor)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        label = self.encoded_labels_tensor[index] #获取已经独热的标签\n",
    "        CWTs = []\n",
    "        path = 'D:/vibration-3classes'\n",
    "        all_files = GetAllFiles(path)\n",
    "        csvpath = all_files[index]\n",
    "        data = pd.read_csv(csvpath)\n",
    "        signal = data['0']\n",
    "        signal = np.array(signal)\n",
    "        imfs,central_freq = GetVMD(signal=signal,K=3) # 输出各个IMF分量，u_hat是各IMF的频谱，omega为各IMF的中心频率\n",
    "        for freq in central_freq:\n",
    "            amp = GetCWTMembers(signal=signal,freq=freq)\n",
    "            CWTs.append(amp)\n",
    "        cwt = np.array(CWTs)\n",
    "        # 返回加载和预处理后的数据\n",
    "        return torch.as_tensor(imfs,dtype=torch.float32), torch.as_tensor(cwt,dtype=torch.float), torch.as_tensor(label,dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = MyDataset()\n",
    "total_size = len(dataset1)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "print(total_size,train_size,val_size,test_size)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset1, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imfs,cwts,label = dataset1[3714]\n",
    "print(imfs)\n",
    "print(imfs.shape)\n",
    "print(cwts)\n",
    "print(cwts.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class VCTmodel(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,num_classes,num_layers1,num_layers2,linear_dim, num_heads1, num_heads2, dropout1, dropout2):\n",
    "        super(VCTmodel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.position_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(hidden_dim, num_layers1, num_heads1, dropout1)\n",
    "        self.transformer_decoder = TransformerDecoder(hidden_dim, num_layers2, num_heads2, dropout2)\n",
    "        self.ClassificationHead = ClassificationHead(hidden_dim,linear_dim,num_classes)\n",
    "\n",
    "    def forward(self, vmd, cwt):\n",
    "\n",
    "        linear_vmd = self.linear(vmd)\n",
    "        pos_vmd = self.position_encoding(linear_vmd)\n",
    "        enc_vmd = self.transformer_encoder(pos_vmd)\n",
    "\n",
    "        linear_cwt = self.linear(cwt)\n",
    "        pos_cwt = self.position_encoding(linear_cwt)\n",
    "        dec_cwt = self.transformer_decoder(enc_vmd,pos_cwt)\n",
    "        output = self.ClassificationHead(dec_cwt)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_length=3):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        position_encoding = torch.zeros(max_length, hidden_dim)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "\n",
    "        position_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('position_encoding', position_encoding.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.position_encoding[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers1, num_heads1, dropout1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_dim, num_heads1, dropout1)\n",
    "            for _ in range(num_layers1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, vmd):\n",
    "        for layer in self.layers:\n",
    "            vmd = layer(vmd)\n",
    "        return vmd\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers2, num_heads2, dropout2):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(hidden_dim, num_heads2, dropout2)\n",
    "            for _ in range(num_layers2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, cwt, transformed):\n",
    "        for layer in self.layers:\n",
    "            cwt = layer(cwt,transformed)\n",
    "        return cwt\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.multihead_attention(x, x, x)[0]\n",
    "        x = x + self.dropout(attended)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        x = x + self.dropout(fed_forward)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, cwt, enc_vmd):\n",
    "        cwt1 = self.multihead_attention(cwt, cwt, cwt)[0]\n",
    "        cwt = cwt + self.dropout(cwt1)\n",
    "        cwt = self.layer_norm1(cwt)\n",
    "\n",
    "        cwt1,= self.multihead_attention(cwt, enc_vmd, enc_vmd)\n",
    "        cwt = cwt + self.dropout(cwt1)\n",
    "        cwt = self.layernorm2(cwt)\n",
    "\n",
    "        fed_forward = self.feed_forward(cwt)\n",
    "        cwt = cwt + self.dropout(fed_forward)\n",
    "        cwt = self.layer_norm3(cwt)\n",
    "\n",
    "        return cwt\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, linear_dim1, linear_dim2, num_classes):\n",
    "        self.linear1 = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 创建模型实例\n",
    "model = TransformerClassifier(input_dim=1024,\n",
    "                              hidden_dim=1024,\n",
    "                              linear_dim=512,\n",
    "                              num_classes=3,\n",
    "                              num_layers=1,\n",
    "                              num_heads=4,\n",
    "                              dropout=0.1\n",
    "                              ).cuda()\n",
    "for param in model.parameters():\n",
    "    print(type(param), param.size())\n",
    "    \n",
    "num_params = sum([param.nelement() for param in model.parameters()])\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Device: {param.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs1, inputs2, labels in tqdm(train_dataloader):\n",
    "        inputs1, inputs2, labels = inputs1.cuda(), inputs2.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            onehotpredicted = torch.zeros(3)\n",
    "            index = predicted[i]\n",
    "            onehotpredicted[index]=1.0\n",
    "            onehotpredicted = onehotpredicted.to('cuda')\n",
    "            if torch.equal(onehotpredicted,labels[i]):\n",
    "                train_correct += 1\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                onehotpredicted = torch.zeros(3)\n",
    "                index = predicted[i]\n",
    "                onehotpredicted[index]=1.0\n",
    "                onehotpredicted = onehotpredicted.to('cuda')\n",
    "                if torch.equal(onehotpredicted,labels[i]):\n",
    "                    correct += 1\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), train_accs, label='Train Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), val_accs, label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_losses,'C:/Users/dell/Desktop/工作文件/论文图/4imfs_trainlosses_1.pt')\n",
    "torch.save(train_accs,'C:/Users/dell/Desktop/工作文件/论文图/4imfs_trainaccs_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 绘制混淆矩阵的函数\n",
    "def plot_confusion_matrix(cm, labels_name, title=\"Confusion Matrix\",  is_norm=True,  colorbar=True, cmap=plt.cm.Blues):\n",
    "    if is_norm==True:\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis],2)  # 横轴归一化并保留2位小数\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)  # 在特定的窗口上显示图像\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            plt.annotate(cm[j, i], xy=(i, j), horizontalalignment='center', verticalalignment='center') # 默认所有值均为黑色\n",
    "            # plt.annotate(cm[j, i], xy=(i, j), horizontalalignment='center', color=\"white\" if i==j else \"black\", verticalalignment='center') # 将对角线值设为白色\n",
    "    if colorbar:\n",
    "        plt.colorbar() # 创建颜色条\n",
    "\n",
    "    num_local = np.array(range(len(labels_name)))\n",
    "    plt.xticks(num_local, labels_name)  # 将标签印在x轴坐标上\n",
    "    plt.yticks(num_local, labels_name)  # 将标签印在y轴坐标上\n",
    "    plt.title(title)  # 图像标题\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show() # plt.show()在plt.savefig()之后\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = torch.tensor([]).cuda()\n",
    "all_labels = torch.tensor([]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_dataloader):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            onehotpredicted = torch.zeros(3)\n",
    "            index = predicted[i]\n",
    "            onehotpredicted[index]=1.0\n",
    "            onehotpredicted = onehotpredicted.to('cuda')\n",
    "            all_preds = torch.cat((all_preds,torch.tensor([index]).cuda()),dim=0)\n",
    "            real_labels = torch.argmax(labels,dim=1)\n",
    "            all_labels = torch.cat((all_labels,torch.tensor([real_labels[i]]).cuda()),dim=0)\n",
    "            if torch.equal(onehotpredicted,labels[i]):\n",
    "                correct += 1\n",
    "# 输出测试集上的损失和准确率\n",
    "print(f\"Test Loss: {test_loss/len(test_dataloader)}, Test Accuracy: {(correct/total)*100}%\")\n",
    "print(correct)\n",
    "print(total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels,all_preds = all_labels.cpu(),all_preds.cpu()\n",
    "\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plot_confusion_matrix(conf_matrix,['Cage', 'Inner race', 'Outer race'],title='cm',is_norm=False)\n",
    "plot_confusion_matrix(conf_matrix,['Cage', 'Inner race', 'Outer race'],title='cm',is_norm=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
